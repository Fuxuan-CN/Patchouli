
import torch
from torch import nn
from torch.utils.data import DataLoader
from pathlib import Path
from typing import Optional
from ..schemas.train_method import TrainMethod
from ..utils.logger import logger
from ..utils.exec_hook import set_exechook
from ..knowledge import CharDataset
from ..brain.tokenizer import PatchouliTokenizer as DefaultTokenizer
from ..schemas.type_hints import ValidatorProtocol, TokenizerProtocol
from pyfiglet import Figlet
from ..information import AUTHOR, DESC, PURPLE_TEXT

class Trainer:
    """ æ¨¡å‹è®­ç»ƒå™¨ """

    def __init__(self, 
        cfg: TrainMethod, 
        model: nn.Module, 
        tokenizer: TokenizerProtocol = DefaultTokenizer(), 
        brain_validator: Optional[ValidatorProtocol] = None,
        default_fatal_hook: bool = True,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scaler: Optional[torch.GradScaler] = None,
        scheduler: Optional[torch.optim.lr_scheduler.LRScheduler] = None
    ) -> None:
        self._print_info()
        if default_fatal_hook: # å¦‚æœå¼€å¯äº†é»˜è®¤é’©å­ï¼Œåˆ™ä¼šè®¾ç½®
            set_exechook()
        logger.debug(f"åˆå§‹åŒ–è®­ç»ƒå™¨...")
        self.validator = brain_validator
        """ éªŒè¯å™¨ """
        self.cfg = cfg
        """ è®­ç»ƒå™¨é…ç½® """
        self.use_cuda = self.cfg.device.startswith("cuda")
        """ æ˜¯å¦ä½¿ç”¨æ˜¾å¡ """
        self.device = self._resolve_device()
        """ è®­ç»ƒç”¨çš„è®¾å¤‡ """
        logger.debug(f"ä½¿ç”¨çš„è®¾å¤‡: {self.device}")
        self.tokenizer = tokenizer
        """ åˆ†è¯å™¨ """
        self.optimizer = optimizer
        """ ä¼˜åŒ–å™¨ """
        self.scaler = scaler
        """ å®šæ ‡å™¨ """
        logger.debug(f"æ¨¡å‹: {model.__class__.__name__} è½½å…¥è®¾å¤‡")
        self.model = model.to(self.device)
        """ è®­ç»ƒçš„æ¨¡å‹ """
        self.accum_steps = self.cfg.gradient_accumulation_steps
        """ æ¢¯åº¦ç´¯è®¡æ­¥æ•° """
        self.scheduler = scheduler
        """ è°ƒåº¦å™¨ """
        logger.debug("è½½å…¥å®Œæˆï¼Œå¼€å§‹è®­ç»ƒé˜¶æ®µ...")
        if not self.validator:
            logger.info("brain_validatorä¸ºç©º, æ²¡æœ‰æ¨¡å‹éªŒè¯å™¨, åç»­è®­ç»ƒå°†é™¤å¼€éªŒè¯æµç¨‹...")
        if self.use_cuda:
            cuda_mem_used = torch.cuda.memory_allocated(self.device) / 1024**3
            logger.success(f"æ¨¡å‹: {self.model.__class__.__name__} è½½å…¥å®Œæˆï¼Œè½½å…¥ä½¿ç”¨çš„æ˜¾å­˜: {cuda_mem_used} GB")

    # ---------------- å†…éƒ¨å·¥å…· ----------------
    def _resolve_device(self) -> str:
        """ è‡ªåŠ¨å¤„ç†åˆé€‚çš„è®¾å¤‡ """
        if self.cfg.device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        
        if self.cfg.device == "cuda" and not torch.cuda.is_available():
            logger.warning("æŒ‡å®š CUDA ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é™çº§åˆ° CPU | å¯èƒ½å‘ç”Ÿæ½œåœ¨çš„æ€§èƒ½ä¸‹é™...")
            return "cpu"

        return self.cfg.device
    
    def _print_info(self) -> None:
        banner = Figlet(font='small').renderText('Patchouli')
        print(f"{PURPLE_TEXT.format(banner=banner)}")
        print(PURPLE_TEXT.format(banner=f"{AUTHOR}: äº²çˆ±çš„ä½œè€…æé†’ä½ ï¼Œ{DESC}\n"))
    
    def _validate(self, epoch: int) -> None:
        """ éªŒè¯æ¨¡å‹ï¼Œå¦‚æœæœ‰æ¨¡å‹éªŒè¯å™¨çš„è¯ """
        if self.validator is not None and epoch % self.cfg.val_every == 0:
            logger.info("å¼€å§‹éªŒè¯...")
            val_loss = self.validator(
                self.model, self._build_dataloader(val=True), self.device, epoch
            )
            logger.info(f"éªŒè¯ç»“æœ: epoch {epoch} | val_loss={val_loss:.4f}")

    def _build_dataloader(self, val: bool = False) -> DataLoader:
        """ æ„å»ºæ•°æ®é›†åŠ è½½å™¨ """
        ds = CharDataset(self.cfg.dataset_path, tokenizer=self.tokenizer, block=self.cfg.block) \
        if not val else CharDataset(self.cfg.val_dataset_path, self.tokenizer, self.cfg.block)

        return DataLoader(
            ds,
            batch_size=self.cfg.batch_size,
            shuffle=self.cfg.loader_shuffle,
            num_workers=self.cfg.loader_num_workers,
            pin_memory=self.use_cuda,
        )
    
    def _save_model_epoch(self, epoch: int) -> None:
        """ ä¿å­˜è®­ç»ƒæ¨¡å‹ """
        ckpt = {
            "epoch": epoch,
            "model_state": self.model.state_dict(),
            "optimizer_state": self.optimizer.state_dict() if self.optimizer else None,
            "scaler_state": self.scaler.state_dict() if self.scaler else None,
            "scheduler": self.scheduler.state_dict() if self.scheduler else None
        }
        epoch_path = Path(self.cfg.export_model_path).with_suffix(f".epoch{epoch}.pth")
        torch.save(ckpt, epoch_path)
        logger.success(f"ğŸ’¾ å·²ä¿å­˜å®Œæ•´ checkpoint {epoch_path.name}")

    def _save_model_final(self) -> None:
        """ ä¿å­˜æœ€ç»ˆæ¨¡å‹ """
        return torch.save(self.model.state_dict(), self.cfg.export_model_path)

    def _judgment_continue_train(self, 
        _continue: Optional[Path]
    ) -> None:
        """ åˆ¤æ–­æ˜¯å¦ç»§ç»­è®­ç»ƒï¼Œå¦‚æœæ˜¯å°†ä¼šåŠ è½½æ¨¡å‹ """
        if _continue and _continue.is_file():
            logger.info(f"ç¡®è®¤äº†ç»§ç»­è®­ç»ƒ, å°†æŒ‡å®šæ¨¡å‹è½½å…¥: {self.device}")
            ckpt = torch.load(_continue, map_location=self.device)
            self.model.load_state_dict(ckpt["model_state"])
            self.optimizer.load_state_dict(ckpt["optimizer_state"]) if self.optimizer else None
            self.scaler.load_state_dict(ckpt["scaler_state"]) if self.scaler else None
            self.scheduler.load_state_dict(ckpt["scheduler"]) if self.scheduler else None
            start_epoch = ckpt["epoch"] + 1
            logger.success(f"ğŸ” è½½å…¥å®Œæˆ, ä¸Šæ¬¡è®­ç»ƒè½®æ•°: {start_epoch - 1}")
    
    def _monitor_system_health(self) -> None:
        """ ç›‘æ§ç³»ç»Ÿå¥åº· """

        if self.use_cuda:
            reserved  = torch.cuda.memory_reserved() / 1024**3
            if reserved > self.cfg.warning_cuda_mem_usage:
                logger.warning(f"æ˜¾å­˜ä½¿ç”¨è¶…è¿‡è­¦æŠ¥é˜ˆå€¼è®¾ç½®: {reserved}GB > {self.cfg.warning_cuda_mem_usage}GB")
                torch.cuda.empty_cache()

    

    # -------------- ä¸»è®­ç»ƒå…¥å£ --------------
    def start(self, continue_from: Optional[str | Path] = None) -> None:
        """ è®­ç»ƒ """
        
        if self.optimizer is None:
            logger.debug("æ²¡æœ‰ä¼ å…¥ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨ ")
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.cfg.learning_rate,
                weight_decay=0.1,
            )

        if self.scaler is None:
            logger.debug("æ²¡æœ‰ä¼ å…¥æ‰“æ ‡å™¨ï¼Œä½¿ç”¨é»˜è®¤æ‰“æ ‡å™¨ ")
            self.scaler = torch.GradScaler(enabled=self.use_cuda)

        _continue = continue_from if isinstance(continue_from, (Path, type(None))) else Path(continue_from)
        dataloader = self._build_dataloader()

        start_epoch = 0
        total_tokens = 0
        epochs = 0

        self._judgment_continue_train(_continue)

        effective_batch = self.cfg.batch_size * self.cfg.gradient_accumulation_steps
        logger.info(
            f"ğŸš€ å¼€å§‹è®­ç»ƒ, å…± {self.cfg.epoch} epoch, "
            f"micro-batch={self.cfg.batch_size}, "
            f"grad-accum={self.cfg.gradient_accumulation_steps}, "
            f"effective-batch={effective_batch}"
        )

        self.model.train()

        try:
            for epoch in range(start_epoch, self.cfg.epoch):
                epoch_loss = 0.0
                epochs = epoch
                self.optimizer.zero_grad()
                for batch_idx, (x, role, y) in enumerate(dataloader):
                    x, role, y = x.to(self.device), role.to(self.device), y.to(self.device)

                    with torch.autocast(
                        enabled=self.device.startswith("cuda"),
                        device_type=self.device,
                    ):
                        logits, new_past, loss = self.model(x, role, y)
                        loss = loss / self.accum_steps   # å¹³å‡åˆ°ç´¯è®¡æ­¥

                    self.scaler.scale(loss).backward()

                    # æ¯ accum_steps æˆ–æœ€åä¸€ä¸ª batch çœŸæ­£æ›´æ–°
                    if (batch_idx + 1) % self.accum_steps == 0 or (batch_idx + 1) == len(dataloader):
                        self.scaler.unscale_(self.optimizer)
                        nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                        self.optimizer.zero_grad()
                        if self.scheduler is not None:
                            self.scheduler.step()

                    tokens_in_batch = y.numel()
                    epoch_loss += loss.item() * tokens_in_batch * self.accum_steps
                    total_tokens += tokens_in_batch

                avg = epoch_loss / max(total_tokens, 1)
                logger.info(f"epoch {epoch} | token avg loss: {avg:.4f}")

                self._validate(epoch)
                # âœ… æ¯ save_every ä¸ª epoch ä¿å­˜ä¸€æ¬¡
                if (epoch + 1) % self.cfg.save_every == 0 or epoch == self.cfg.epoch - 1:
                    if self.cfg.save_model_training: # å¦‚æœè®¾å®šäº†è®­ç»ƒè¦ä¿å­˜æ¨¡å‹ï¼Œåˆ™ä¼šä¿å­˜
                        self._save_model_epoch(epoch)

                if (epoch + 1) % self.cfg.sys_health_check_every == 0 or epoch == self.cfg.sys_health_check_every - 1:
                    self._monitor_system_health()

        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ç»ˆæ­¢äº†è®­ç»ƒç¨‹åº...")
            if self.cfg.interrupt_save:
                logger.info("æ­£åœ¨ä¿å­˜æ¨¡å‹...")
                self._save_model_epoch(epochs)
        else:
            # è®­ç»ƒå®Œæˆä¼šè¿›å…¥æ­¤å—ä¿å­˜æ¨¡å‹
            logger.info("æ‰€æœ‰è½®æ¬¡è®­ç»ƒå®Œæˆ, å¯¼å‡ºæœ€ç»ˆæƒé‡...")
            self._save_model_final()
            logger.success(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæƒé‡å·²ç»å¯¼å‡ºåˆ°: {self.cfg.export_model_path}")